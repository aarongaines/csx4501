{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04bfa1b8-7c3a-4fdc-bb21-a42ef07591e0",
   "metadata": {},
   "source": [
    "# ML training:  using a test/train split and cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d00c71-08b2-4eb6-8d92-aa0481ba60a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de08088e-0e3e-42d8-8ca9-b888dac52e9e",
   "metadata": {},
   "source": [
    "### Get the data\n",
    "Here we're manufacturing it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904a6333-8acf-48bf-9453-f05a1e36d98e",
   "metadata": {},
   "source": [
    "We're going to generate some fictitious data that follows an equation of our choosing:\n",
    "\n",
    "$$ y(x) = 4 + 2x - x^2 + 0.075x^3 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41d89cb-cece-461b-bfa3-32bde7c6ef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 10, 50)\n",
    "y = 4 + 2*x - x**2 + 0.075*x**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4baf8d0-ba40-4058-8e13-d23f589d68dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x,y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217e084b-e62b-47c2-8af8-d1426d429082",
   "metadata": {},
   "source": [
    "### Real data is noisy\n",
    "\n",
    "There's always noise in data.  It may be noise from measurement, or \"noise\" in the sense that there are aspects of the data that aren't captured by the features that we measure.\n",
    "\n",
    "Let's introduce some random noise into our target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87be26f7-dce8-440e-941d-a9c898d4da36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 20 points from a normal \n",
    "# distribution that has mean = 0 and std dev = 1.5\n",
    "np.random.seed(42)\n",
    "noise = np.random.normal(0,1.5,50)\n",
    "\n",
    "x = np.linspace(0, 10, 50)\n",
    "\n",
    "# this y is the theoretical value + noise\n",
    "y_with_noise = 4 + 2*x - x**2 + 0.075*x**3 + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a13fe04-d510-484d-ae23-15d61cbd52f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot our theory curve\n",
    "plt.plot(x,y,'k')\n",
    "\n",
    "# plot our data generated from the theory curve + noise\n",
    "plt.scatter(x,y_with_noise,color='k',marker='o')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ebdd56-c190-4bbe-961e-f300e871cd4f",
   "metadata": {},
   "source": [
    "## ML training process\n",
    "\n",
    "* get the data and pre-process if needed\n",
    "* choose the model\n",
    "* train the model\n",
    "* evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc85662-6aaa-4094-9c67-7e70771d26d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data if needed\n",
    "\n",
    "# Here we need to make the 1D numpy array for x into \n",
    "# a 2D numpy array with 1 feature column\n",
    "x_transformed = x.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b80324-5b66-47df-958a-b6b2a36a7e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the model\n",
    "\n",
    "import sklearn.linear_model\n",
    "model = sklearn.linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67bbcd9-4370-4a77-b18c-b709e3407d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "model.fit(x_transformed, y_with_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f088a32-5843-4a9c-9527-1577baa61626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "# You need to evaluate your model, so look at\n",
    "# what it will predict for the outcome variable\n",
    "y_pred = model.predict(x_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa253dc-7315-41e9-ad3d-e1f646163dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot our data generated from the theory curve + noise\n",
    "plt.scatter(x,y_with_noise,color='black')\n",
    "\n",
    "# plot the trained model on a range of x-values over all space\n",
    "x_model_vals = np.linspace(0, 10, 50).reshape(-1,1)\n",
    "y_model_vals = model.predict(x_model_vals)\n",
    "plt.plot(x,y_model_vals,'blue')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0352dc7-c7d5-41ca-ac8b-244e8b009d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the model\n",
    "print('Intercept = %.2f' % model.intercept_)\n",
    "print('Model coefficients = %.2f' % model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d400ed17-f721-4c5c-a434-41475411120e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the MSE of the predictions relative to \n",
    "# the true y values of the data\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "print('MSE = %.2f' % mean_squared_error(y_with_noise, y_pred))\n",
    "print('R^2 = %.2f' % r2_score(y_with_noise, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c71180a-ab6e-4bc8-9435-38543f79d16c",
   "metadata": {},
   "source": [
    "## Example of instance-based model:  K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d693890c-9381-4e74-ac5d-423953b4090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the model\n",
    "\n",
    "import sklearn.neighbors\n",
    "model = sklearn.neighbors.KNeighborsRegressor(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0723930-5259-4ca8-87c5-2b3c0028133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "model.fit(x_transformed, y_with_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c60111c-de2d-4d5c-a3ea-0673c7eccfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "# You need to evaluate your model, so look at\n",
    "# what it will predict for the outcome variable\n",
    "y_pred = model.predict(x_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad177a3-78a9-4092-a8fe-42732b654a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot our data generated from the theory curve + noise\n",
    "plt.scatter(x,y_with_noise,color='black')\n",
    "\n",
    "# plot the trained model on a range of x-values over all space\n",
    "x_model_vals = np.linspace(0, 10, 50).reshape(-1,1)\n",
    "y_model_vals = model.predict(x_model_vals)\n",
    "plt.plot(x,y_model_vals,'blue')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974c5b88-ab14-4815-b962-520f1a7f1ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the MSE of the predictions relative to \n",
    "# the true y values of the data\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "print('MSE = %.2f' % mean_squared_error(y_with_noise, y_pred))\n",
    "print('R^2 = %.2f' % r2_score(y_with_noise, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f6a2a5-d413-4ea8-954c-e9a2b60ef673",
   "metadata": {},
   "source": [
    "### Train/test split\n",
    "\n",
    "If we evaluate our model by quantifying how well it fits the same data that we used to train it, then we risk making a biased judgement about how good it is.  \n",
    "\n",
    "Our model was explicitly designed to match the data it's trained on.  If we want to know how well it will generalize to making predictions about new data, we should evaluate it on new data.\n",
    "\n",
    "Solution to this problem -> reserve some of our data (with labels we already know) so that we can use it as \"unseen\" data when evaluating our model after it's been trained.\n",
    "* Split into data for training and data for testing\n",
    "* Below we take our initial data (x, y_with_noise) and we split it up so that 20% of it will not be used in training and can instead be used to evaluate the model at the end\n",
    "* (x_train, y_train) -> the data used to train the model\n",
    "* (x_test, y_test) -> the data used to evaluate the model after training\n",
    "* `random_state` is specified so that we can get a repeatable split of data in case we return to do this multiple times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d0599e-94c0-44b6-8987-edd830e2c51a",
   "metadata": {},
   "source": [
    "## Generalize this\n",
    "\n",
    "The above process is independent of the particular algorithm that we chose to use for our ML.\n",
    "\n",
    "We are going to do both Linear Regression and k-Nearest Neighbors Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3a3b6a-fb42-4aa2-9cd9-16268820e81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "import sklearn.neighbors\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307f6b55-fc9c-48ef-baf4-a0b7bad1c050",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = sklearn.linear_model.LinearRegression()\n",
    "model2 = sklearn.neighbors.KNeighborsRegressor(n_neighbors=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "    x, y_with_noise, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train_transformed = x_train.reshape(-1,1)\n",
    "x_test_transformed = x_test.reshape(-1,1)\n",
    "\n",
    "model1.fit(x_train_transformed,\n",
    "          y_train)\n",
    "model2.fit(x_train_transformed,\n",
    "          y_train)\n",
    "\n",
    "plt.scatter(x_train,y_train,color='black')\n",
    "plt.scatter(x_test,y_test,color='blue')\n",
    "\n",
    "x_model_vals = np.linspace(0, 10, 50).reshape(-1,1)\n",
    "y_model1_vals = model1.predict(x_model_vals)\n",
    "y_model2_vals = model2.predict(x_model_vals)\n",
    "plt.plot(x,y_model1_vals,'red')\n",
    "plt.plot(x,y_model2_vals,'green')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "y_pred1 = model1.predict(x_test_transformed)\n",
    "y_pred2 = model2.predict(x_test_transformed)\n",
    "\n",
    "print('MSE_linreg = %.2f' % mean_squared_error(y_test, y_pred1))\n",
    "print('MSE_knn = %.2f' % mean_squared_error(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0a15ef-ddbf-496c-a3d9-ea57219f2aa9",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Hyperparameters are parameters of the particular ML model that we train, and are different from the parameters of the underlying model that is being trained.\n",
    "* The slope and intercept of a line are parameters that are learned during the process of linear regression.  \n",
    "* The number of neighbors that get used during kNN is a hyperparameter that gets specified before the training is performed and it is not a parameter whose value is learned from the data.\n",
    "\n",
    "The ML model can have different levels of performance for different values of hyperparameters.\n",
    "\n",
    "Let's see which value of nearest neighbors gives the best performance here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd3f254-0930-4a3c-b285-63b59c13f13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knntest(n=1):\n",
    "    model2 = sklearn.neighbors.KNeighborsRegressor(n_neighbors=n)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "        x, y_with_noise, test_size=0.2, random_state=42)\n",
    "\n",
    "    x_train_transformed = x_train.reshape(-1,1)\n",
    "    x_test_transformed = x_test.reshape(-1,1)\n",
    "\n",
    "    model2.fit(x_train_transformed,\n",
    "              y_train)\n",
    "\n",
    "    y_pred2 = model2.predict(x_test_transformed)\n",
    "\n",
    "    print('n_neighbers = {:02d} : MSE_knn = {:.2f}'.format(n, mean_squared_error(y_test, y_pred2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e477cd-4856-4ba6-9482-d02586a99f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,15):\n",
    "    knntest(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad42aefc-ed75-4b1c-8723-105c426eb751",
   "metadata": {},
   "source": [
    "We can use the above to select a value of the hyperparameter that works the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86576a47-5fb3-4fc2-a8db-c8814438e5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knntest(n=1):\n",
    "    model2 = sklearn.neighbors.KNeighborsRegressor(n_neighbors=n)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "        x, y_with_noise, test_size=0.2, random_state=42)\n",
    "\n",
    "    x_train_transformed = x_train.reshape(-1,1)\n",
    "    x_test_transformed = x_test.reshape(-1,1)\n",
    "\n",
    "    model2.fit(x_train_transformed,\n",
    "              y_train)\n",
    "\n",
    "    y_predtrain = model2.predict(x_train_transformed)\n",
    "    y_predtest = model2.predict(x_test_transformed)\n",
    "\n",
    "    return(n,\n",
    "           mean_squared_error(y_train, y_predtrain),\n",
    "           mean_squared_error(y_test, y_predtest))\n",
    "    \n",
    "kscores_nneighbors = []\n",
    "kscores_train = []\n",
    "kscores_test = []\n",
    "for i in range(1,15):\n",
    "    result = knntest(i)\n",
    "    kscores_nneighbors.append(result[0])\n",
    "    kscores_train.append(result[1])\n",
    "    kscores_test.append(result[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f19d745-5934-4b6e-820c-1e4b6a5530f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([1/i for i in kscores_nneighbors], kscores_train, \n",
    "         [1/i for i in kscores_nneighbors], kscores_test,\n",
    "         marker='o',markersize=4)\n",
    "plt.xlabel('Flexibility (here 1/n_neighbors)')\n",
    "plt.legend(['train error','test error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47deeee2-f6e9-4be7-8cc7-00dd91c51b74",
   "metadata": {},
   "source": [
    "## Potential problems\n",
    "\n",
    "If we evaluate our model on data that has been used to train the model, then we run the risk of fitting our model too closely to the specifics of the data.\n",
    "\n",
    "Likewise, if we determine the ideal values of our hyperparameters based on how they perform on the test data, we run the risk of fitting our hyperparameters too closely to the specifics of our test data.\n",
    "\n",
    "Solution to this problem -> cross-validation:\n",
    "* split the training data up into K folds, where K is an arbitrarily chosen number\n",
    "* train the model K times, each time holding out a different subset to use for testing and training on the remaining K-1 subsets\n",
    "* average the K resulting scores\n",
    "* repeat as needed for various values of hyperparameters\n",
    "* chose the hyperparameter with the best results in the cross-validation process, and assess it in the final stage by training on the full training data and testing against the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7049fcf5-2871-4b0c-aafb-85bb9deac0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb037b45-bf61-4d88-aa1c-1394f4048e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = sklearn.neighbors.KNeighborsRegressor(n_neighbors=3)\n",
    "\n",
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "    x, y_with_noise, test_size=0.2, random_state=42)\n",
    "x_train_transformed = x_train.reshape(-1,1)\n",
    "\n",
    "loss = cross_val_score(model2,\n",
    "                       x_train_transformed,\n",
    "                       y_train, \n",
    "                       cv=5, \n",
    "                       scoring='neg_mean_squared_error')\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b73c555-86e9-4615-b909-62333b5862a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.sqrt(-loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9f990c-f110-4454-90db-d30e324bde2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "        x, y_with_noise, test_size=0.2, random_state=42)\n",
    "x_train_transformed = x_train.reshape(-1,1)\n",
    "x_test_transformed = x_test.reshape(-1,1)\n",
    "\n",
    "k_range = range(1, 20)\n",
    "k_scores = []\n",
    "for k in k_range:\n",
    "    knn = sklearn.neighbors.KNeighborsRegressor(n_neighbors=k)\n",
    "    loss = cross_val_score(knn,\n",
    "                           x_train_transformed,\n",
    "                           y_train, \n",
    "                           cv=5, \n",
    "                           scoring='neg_mean_squared_error')\n",
    "    k_scores.append(np.sqrt(-loss).mean())\n",
    "plt.scatter(k_range, k_scores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-Validated MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c4c5a3-a510-4f8b-a858-5c996e137d06",
   "metadata": {},
   "source": [
    "Now we have our best guess for a hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307e254a-aff3-4794-95b7-fbfa97a9da82",
   "metadata": {},
   "outputs": [],
   "source": [
    "knntest(n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d12dc5-91bd-461d-b81f-3fc3cbbdd7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.neighbors.KNeighborsRegressor(n_neighbors=4)\n",
    "\n",
    "model.fit(x_train_transformed, y_train)\n",
    "\n",
    "plt.plot(x,y,'k')\n",
    "plt.scatter(x_train,y_train,color='black')\n",
    "plt.scatter(x_test,y_test,color='blue')\n",
    "\n",
    "x_model_vals = np.linspace(0, 10, 50).reshape(-1,1)\n",
    "y_model_vals = model.predict(x_model_vals)\n",
    "plt.plot(x,y_model_vals,'green')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "y_pred = model.predict(x_test_transformed)\n",
    "\n",
    "print('MSE_knn = ', mean_squared_error(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
